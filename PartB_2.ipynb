{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFVxWZGJxprU"
   },
   "source": [
    "# CS4001/4042 Assignment 1, Part B, Q2\n",
    "In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EycCozG06Duu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-widedeep\n",
      "  Obtaining dependency information for pytorch-widedeep from https://files.pythonhosted.org/packages/17/f4/48f8d4c527baea10808b822fd3c00260f2b3b453937f2ef54bc464da1b88/pytorch_widedeep-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading pytorch_widedeep-1.3.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas>=1.3.5 in ./venv/lib/python3.8/site-packages (from pytorch-widedeep) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.21.6 in ./venv/lib/python3.8/site-packages (from pytorch-widedeep) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.7.3 in ./venv/lib/python3.8/site-packages (from pytorch-widedeep) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in ./venv/lib/python3.8/site-packages (from pytorch-widedeep) (1.3.0)\n",
      "Collecting gensim (from pytorch-widedeep)\n",
      "  Obtaining dependency information for gensim from https://files.pythonhosted.org/packages/0c/a7/2dd786427bedd2c3dc6c74b70e1e53c6c180a7da0a686c61c2ab17f6fc63/gensim-4.3.2-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading gensim-4.3.2-cp38-cp38-macosx_10_9_x86_64.whl.metadata (8.5 kB)\n",
      "Collecting spacy (from pytorch-widedeep)\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/2c/f5/4aacdbc74b0bfbb485a63a2b1d2982c2fde53702b7cd8b19d9db2ae7bb18/spacy-3.6.1-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading spacy-3.6.1-cp38-cp38-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting opencv-contrib-python (from pytorch-widedeep)\n",
      "  Obtaining dependency information for opencv-contrib-python from https://files.pythonhosted.org/packages/d7/c1/8807b1145c89f64734517ab18f0acc11e021059cd8fdf5c765f4633ae0bc/opencv_contrib_python-4.8.1.78-cp37-abi3-macosx_10_16_x86_64.whl.metadata\n",
      "  Downloading opencv_contrib_python-4.8.1.78-cp37-abi3-macosx_10_16_x86_64.whl.metadata (19 kB)\n",
      "Collecting imutils (from pytorch-widedeep)\n",
      "  Downloading imutils-0.5.4.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm in ./venv/lib/python3.8/site-packages (from pytorch-widedeep) (4.66.1)\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.8/site-packages (from pytorch-widedeep) (1.13.1)\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.8/site-packages (from pytorch-widedeep) (0.15.2)\n",
      "Requirement already satisfied: einops in ./venv/lib/python3.8/site-packages (from pytorch-widedeep) (0.6.1)\n",
      "Collecting wrapt (from pytorch-widedeep)\n",
      "  Downloading wrapt-1.15.0-cp38-cp38-macosx_10_9_x86_64.whl (35 kB)\n",
      "Requirement already satisfied: torchmetrics in ./venv/lib/python3.8/site-packages (from pytorch-widedeep) (0.11.4)\n",
      "Collecting pyarrow (from pytorch-widedeep)\n",
      "  Obtaining dependency information for pyarrow from https://files.pythonhosted.org/packages/5d/9e/4e2306bbb2388b7d9c6ea8a830a31b5827ec279423d6e120acf95a3fb484/pyarrow-13.0.0-cp38-cp38-macosx_10_14_x86_64.whl.metadata\n",
      "  Downloading pyarrow-13.0.0-cp38-cp38-macosx_10_14_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting fastparquet>=0.8.1 (from pytorch-widedeep)\n",
      "  Obtaining dependency information for fastparquet>=0.8.1 from https://files.pythonhosted.org/packages/d2/3a/3ebe290aabab77136f41d7d1b49b54911fe8ab3aae108c935bc957191432/fastparquet-2023.8.0-cp38-cp38-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading fastparquet-2023.8.0-cp38-cp38-macosx_10_9_universal2.whl.metadata (4.1 kB)\n",
      "Collecting cramjam>=2.3 (from fastparquet>=0.8.1->pytorch-widedeep)\n",
      "  Obtaining dependency information for cramjam>=2.3 from https://files.pythonhosted.org/packages/4a/b9/59cb1e919ca90b62b092701a24a5cfe74605425370643dd2809af6d53f90/cramjam-2.7.0-cp38-cp38-macosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl.metadata\n",
      "  Downloading cramjam-2.7.0-cp38-cp38-macosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.8/site-packages (from fastparquet>=0.8.1->pytorch-widedeep) (2023.9.2)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.8/site-packages (from fastparquet>=0.8.1->pytorch-widedeep) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.8/site-packages (from pandas>=1.3.5->pytorch-widedeep) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.8/site-packages (from pandas>=1.3.5->pytorch-widedeep) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./venv/lib/python3.8/site-packages (from pandas>=1.3.5->pytorch-widedeep) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./venv/lib/python3.8/site-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.8/site-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (3.2.0)\n",
      "Collecting smart-open>=1.8.1 (from gensim->pytorch-widedeep)\n",
      "  Obtaining dependency information for smart-open>=1.8.1 from https://files.pythonhosted.org/packages/fc/d9/d97f1db64b09278aba64e8c81b5d322d436132df5741c518f3823824fae0/smart_open-6.4.0-py3-none-any.whl.metadata\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy->pytorch-widedeep)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy->pytorch-widedeep)\n",
      "  Obtaining dependency information for spacy-loggers<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/33/78/d1a1a026ef3af911159398c939b1509d5c36fe524c7b644f34a5146c4e16/spacy_loggers-1.0.5-py3-none-any.whl.metadata\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy->pytorch-widedeep)\n",
      "  Obtaining dependency information for murmurhash<1.1.0,>=0.28.0 from https://files.pythonhosted.org/packages/52/32/83c7d19b8017bae29a22e6d691452236c5532c478069b74cb56c15786338/murmurhash-1.0.10-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading murmurhash-1.0.10-cp38-cp38-macosx_10_9_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy->pytorch-widedeep)\n",
      "  Obtaining dependency information for cymem<2.1.0,>=2.0.2 from https://files.pythonhosted.org/packages/56/e0/d3b5727fd7650bc9617edd8dd982405adb44a012e19890972608ff53afa0/cymem-2.0.8-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading cymem-2.0.8-cp38-cp38-macosx_10_9_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy->pytorch-widedeep)\n",
      "  Obtaining dependency information for preshed<3.1.0,>=3.0.2 from https://files.pythonhosted.org/packages/58/bc/972e89282e93d97bcc566461ea79d6663cabc1ecae78634dd9d7db604ac9/preshed-3.0.9-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading preshed-3.0.9-cp38-cp38-macosx_10_9_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.8 (from spacy->pytorch-widedeep)\n",
      "  Obtaining dependency information for thinc<8.2.0,>=8.1.8 from https://files.pythonhosted.org/packages/00/e1/6cbcf20fc215f1b358d6a04672bec122b26da7293c4686c5329ecdc6f105/thinc-8.1.12-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading thinc-8.1.12-cp38-cp38-macosx_10_9_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy->pytorch-widedeep)\n",
      "  Obtaining dependency information for wasabi<1.2.0,>=0.9.1 from https://files.pythonhosted.org/packages/8f/69/26cbf0bad11703241cb84d5324d868097f7a8faf2f1888354dac8883f3fc/wasabi-1.1.2-py3-none-any.whl.metadata\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy->pytorch-widedeep)\n",
      "  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/82/41/4d759a425297672e1e243b304aa32ffaad945d4fe3977b3bf38dac3af7a8/srsly-2.4.8-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading srsly-2.4.8-cp38-cp38-macosx_10_9_x86_64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy->pytorch-widedeep)\n",
      "  Obtaining dependency information for catalogue<2.1.0,>=2.0.6 from https://files.pythonhosted.org/packages/9e/96/d32b941a501ab566a16358d68b6eb4e4acc373fab3c3c4d7d9e649f7b4bb/catalogue-2.0.10-py3-none-any.whl.metadata\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy->pytorch-widedeep)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pathy>=0.10.0 (from spacy->pytorch-widedeep)\n",
      "  Obtaining dependency information for pathy>=0.10.0 from https://files.pythonhosted.org/packages/b5/c3/04a002ace658133f5ac48d30258ed9ceab720595dc1ac36df02fe52018af/pathy-0.10.2-py3-none-any.whl.metadata\n",
      "  Downloading pathy-0.10.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.8/site-packages (from spacy->pytorch-widedeep) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy->pytorch-widedeep)\n",
      "  Obtaining dependency information for pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 from https://files.pythonhosted.org/packages/73/66/0a72c9fcde42e5650c8d8d5c5c1873b9a3893018020c77ca8eb62708b923/pydantic-2.4.2-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.4.2-py3-none-any.whl.metadata (158 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.6/158.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in ./venv/lib/python3.8/site-packages (from spacy->pytorch-widedeep) (3.1.2)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from spacy->pytorch-widedeep) (49.2.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy->pytorch-widedeep)\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in ./venv/lib/python3.8/site-packages (from torch->pytorch-widedeep) (4.7.1)\n",
      "Collecting torch (from pytorch-widedeep)\n",
      "  Using cached torch-2.0.1-cp38-none-macosx_10_9_x86_64.whl (143.1 MB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.8/site-packages (from torchvision->pytorch-widedeep) (10.0.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.8/site-packages (from torch->pytorch-widedeep) (3.12.4)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.8/site-packages (from torch->pytorch-widedeep) (1.12)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.8/site-packages (from torch->pytorch-widedeep) (3.1)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->pytorch-widedeep)\n",
      "  Obtaining dependency information for annotated-types>=0.4.0 from https://files.pythonhosted.org/packages/d8/f0/a2ee543a96cc624c35a9086f39b1ed2aa403c6d355dfe47a11ee5c64a164/annotated_types-0.5.0-py3-none-any.whl.metadata\n",
      "  Downloading annotated_types-0.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pydantic-core==2.10.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->pytorch-widedeep)\n",
      "  Obtaining dependency information for pydantic-core==2.10.1 from https://files.pythonhosted.org/packages/be/cc/66adc75728ab224d7907cbea1c56c6ae52dfedc32a7097aec9a139e41f70/pydantic_core-2.10.1-cp38-cp38-macosx_10_7_x86_64.whl.metadata\n",
      "  Downloading pydantic_core-2.10.1-cp38-cp38-macosx_10_7_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->pytorch-widedeep) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (2023.7.22)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.2.0,>=8.1.8->spacy->pytorch-widedeep)\n",
      "  Obtaining dependency information for blis<0.8.0,>=0.7.8 from https://files.pythonhosted.org/packages/6d/88/bd90a1c3f7dd3cab95d41404932a0926bf2dc54f6cc28cc27feb9480aa4e/blis-0.7.11-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading blis-0.7.11-cp38-cp38-macosx_10_9_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.2.0,>=8.1.8->spacy->pytorch-widedeep)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/93/f8/e89268a1f885048fb2ee6b5c9f93c4e90de768534acfef3652f87d97d4cb/confection-0.1.3-py3-none-any.whl.metadata\n",
      "  Downloading confection-0.1.3-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./venv/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy->pytorch-widedeep) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.8/site-packages (from jinja2->spacy->pytorch-widedeep) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.8/site-packages (from sympy->torch->pytorch-widedeep) (1.3.0)\n",
      "Downloading pytorch_widedeep-1.3.2-py3-none-any.whl (21.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/21.8 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading fastparquet-2023.8.0-cp38-cp38-macosx_10_9_universal2.whl (911 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m911.6/911.6 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gensim-4.3.2-cp38-cp38-macosx_10_9_x86_64.whl (24.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_contrib_python-4.8.1.78-cp37-abi3-macosx_10_16_x86_64.whl (64.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-13.0.0-cp38-cp38-macosx_10_14_x86_64.whl (25.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.9/25.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading spacy-3.6.1-cp38-cp38-macosx_10_9_x86_64.whl (6.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cramjam-2.7.0-cp38-cp38-macosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading cymem-2.0.8-cp38-cp38-macosx_10_9_x86_64.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading murmurhash-1.0.10-cp38-cp38-macosx_10_9_x86_64.whl (26 kB)\n",
      "Downloading pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading preshed-3.0.9-cp38-cp38-macosx_10_9_x86_64.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.4.2-py3-none-any.whl (395 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.8/395.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.10.1-cp38-cp38-macosx_10_7_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp38-cp38-macosx_10_9_x86_64.whl (490 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.9/490.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.1.12-cp38-cp38-macosx_10_9_x86_64.whl (857 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.6/857.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
      "Downloading blis-0.7.11-cp38-cp38-macosx_10_9_x86_64.whl (6.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading confection-0.1.3-py3-none-any.whl (34 kB)\n",
      "Building wheels for collected packages: imutils\n",
      "  Building wheel for imutils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25860 sha256=d5d26ad3119a2ac3387aebe43b6a99879edea6321d0c80ddec5e4c63b9c7eb68\n",
      "  Stored in directory: /Users/tohhongxiang/Library/Caches/pip/wheels/59/1b/52/0dea905f8278d5514dc4d0be5e251967f8681670cadd3dca89\n",
      "Successfully built imutils\n",
      "Installing collected packages: imutils, cymem, wrapt, wasabi, typer, spacy-loggers, spacy-legacy, smart-open, pydantic-core, pyarrow, opencv-contrib-python, murmurhash, langcodes, cramjam, catalogue, blis, annotated-types, torch, srsly, pydantic, preshed, pathy, gensim, fastparquet, confection, thinc, spacy, pytorch-widedeep\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.13.1\n",
      "    Uninstalling torch-1.13.1:\n",
      "      Successfully uninstalled torch-1.13.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytorch-tabnet 4.0 requires torch<2.0,>=1.2, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.5.0 blis-0.7.11 catalogue-2.0.10 confection-0.1.3 cramjam-2.7.0 cymem-2.0.8 fastparquet-2023.8.0 gensim-4.3.2 imutils-0.5.4 langcodes-3.3.0 murmurhash-1.0.10 opencv-contrib-python-4.8.1.78 pathy-0.10.2 preshed-3.0.9 pyarrow-13.0.0 pydantic-2.4.2 pydantic-core-2.10.1 pytorch-widedeep-1.3.2 smart-open-6.4.0 spacy-3.6.1 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.1.12 torch-2.0.1 typer-0.9.0 wasabi-1.1.2 wrapt-1.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-widedeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lq0elU0J53Yo"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
    "from pytorch_widedeep.models import TabMlp, WideDeep\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.metrics import R2Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU3xdVpwzuLx"
   },
   "source": [
    ">Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_oYG6lNIh7Mp"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "\n",
    "# TODO: Enter your code here\n",
    "df_train = df[df['year'] <= 2020]\n",
    "df_test = df[df['year'] >= 2021]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_q9PoR50JAA"
   },
   "source": [
    ">Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n",
    "https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n",
    "* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n",
    "features and the categorical features. Use this component to transform the training dataset.\n",
    "* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 linear layers in the MLP, with 200 and 100 neurons respectively.\n",
    "* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 100 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZBY1iqUXtYWn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tohhongxiang/Desktop/programming/CZ4042-Assignment/venv/lib/python3.8/site-packages/pytorch_widedeep/preprocessing/tab_preprocessor.py:334: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1: 100%|██████████| 1366/1366 [00:11<00:00, 118.54it/s, loss=2.25e+5, metrics={'r2': -2.174}] \n",
      "epoch 2: 100%|██████████| 1366/1366 [00:11<00:00, 119.50it/s, loss=9.64e+4, metrics={'r2': 0.5599}]\n",
      "epoch 3: 100%|██████████| 1366/1366 [00:11<00:00, 118.96it/s, loss=8.55e+4, metrics={'r2': 0.6624}]\n",
      "epoch 4: 100%|██████████| 1366/1366 [00:11<00:00, 121.52it/s, loss=7.95e+4, metrics={'r2': 0.7135}]\n",
      "epoch 5: 100%|██████████| 1366/1366 [00:11<00:00, 121.49it/s, loss=7.56e+4, metrics={'r2': 0.7437}]\n",
      "epoch 6: 100%|██████████| 1366/1366 [00:11<00:00, 121.27it/s, loss=7.33e+4, metrics={'r2': 0.7604}]\n",
      "epoch 7: 100%|██████████| 1366/1366 [00:11<00:00, 120.96it/s, loss=7.18e+4, metrics={'r2': 0.7709}]\n",
      "epoch 8: 100%|██████████| 1366/1366 [00:11<00:00, 119.88it/s, loss=7.06e+4, metrics={'r2': 0.7789}]\n",
      "epoch 9: 100%|██████████| 1366/1366 [00:11<00:00, 120.63it/s, loss=6.93e+4, metrics={'r2': 0.7866}]\n",
      "epoch 10: 100%|██████████| 1366/1366 [00:11<00:00, 115.67it/s, loss=6.86e+4, metrics={'r2': 0.7904}]\n",
      "epoch 11: 100%|██████████| 1366/1366 [00:11<00:00, 116.18it/s, loss=6.76e+4, metrics={'r2': 0.797}] \n",
      "epoch 12: 100%|██████████| 1366/1366 [00:11<00:00, 114.33it/s, loss=6.74e+4, metrics={'r2': 0.7981}]\n",
      "epoch 13: 100%|██████████| 1366/1366 [00:12<00:00, 108.80it/s, loss=6.68e+4, metrics={'r2': 0.8017}]\n",
      "epoch 14: 100%|██████████| 1366/1366 [00:11<00:00, 114.40it/s, loss=6.64e+4, metrics={'r2': 0.8031}]\n",
      "epoch 15: 100%|██████████| 1366/1366 [00:11<00:00, 117.41it/s, loss=6.58e+4, metrics={'r2': 0.8071}]\n",
      "epoch 16: 100%|██████████| 1366/1366 [00:11<00:00, 116.47it/s, loss=6.58e+4, metrics={'r2': 0.8068}]\n",
      "epoch 17: 100%|██████████| 1366/1366 [00:13<00:00, 103.72it/s, loss=6.55e+4, metrics={'r2': 0.8084}]\n",
      "epoch 18: 100%|██████████| 1366/1366 [00:12<00:00, 109.77it/s, loss=6.52e+4, metrics={'r2': 0.8098}]\n",
      "epoch 19: 100%|██████████| 1366/1366 [00:10<00:00, 127.13it/s, loss=6.48e+4, metrics={'r2': 0.8123}]\n",
      "epoch 20: 100%|██████████| 1366/1366 [00:10<00:00, 134.06it/s, loss=6.48e+4, metrics={'r2': 0.8124}]\n",
      "epoch 21: 100%|██████████| 1366/1366 [00:10<00:00, 133.37it/s, loss=6.46e+4, metrics={'r2': 0.813}] \n",
      "epoch 22: 100%|██████████| 1366/1366 [00:10<00:00, 128.94it/s, loss=6.42e+4, metrics={'r2': 0.815}] \n",
      "epoch 23: 100%|██████████| 1366/1366 [00:12<00:00, 105.70it/s, loss=6.41e+4, metrics={'r2': 0.8157}]\n",
      "epoch 24: 100%|██████████| 1366/1366 [00:13<00:00, 102.92it/s, loss=6.38e+4, metrics={'r2': 0.8175}]\n",
      "epoch 25: 100%|██████████| 1366/1366 [00:13<00:00, 102.55it/s, loss=6.37e+4, metrics={'r2': 0.8181}]\n",
      "epoch 26: 100%|██████████| 1366/1366 [00:10<00:00, 125.98it/s, loss=6.35e+4, metrics={'r2': 0.8189}]\n",
      "epoch 27: 100%|██████████| 1366/1366 [00:12<00:00, 105.70it/s, loss=6.34e+4, metrics={'r2': 0.8195}]\n",
      "epoch 28: 100%|██████████| 1366/1366 [00:14<00:00, 97.47it/s, loss=6.32e+4, metrics={'r2': 0.8209}] \n",
      "epoch 29: 100%|██████████| 1366/1366 [00:13<00:00, 104.08it/s, loss=6.32e+4, metrics={'r2': 0.82}]  \n",
      "epoch 30: 100%|██████████| 1366/1366 [00:13<00:00, 104.08it/s, loss=6.28e+4, metrics={'r2': 0.8222}]\n",
      "epoch 31: 100%|██████████| 1366/1366 [00:13<00:00, 99.56it/s, loss=6.26e+4, metrics={'r2': 0.8233}] \n",
      "epoch 32: 100%|██████████| 1366/1366 [00:13<00:00, 102.10it/s, loss=6.27e+4, metrics={'r2': 0.8232}]\n",
      "epoch 33: 100%|██████████| 1366/1366 [00:12<00:00, 112.03it/s, loss=6.26e+4, metrics={'r2': 0.8238}]\n",
      "epoch 34: 100%|██████████| 1366/1366 [00:13<00:00, 102.49it/s, loss=6.25e+4, metrics={'r2': 0.8239}]\n",
      "epoch 35: 100%|██████████| 1366/1366 [00:13<00:00, 103.50it/s, loss=6.22e+4, metrics={'r2': 0.8259}]\n",
      "epoch 36: 100%|██████████| 1366/1366 [00:13<00:00, 103.36it/s, loss=6.21e+4, metrics={'r2': 0.8257}]\n",
      "epoch 37: 100%|██████████| 1366/1366 [00:13<00:00, 104.13it/s, loss=6.19e+4, metrics={'r2': 0.827}] \n",
      "epoch 38: 100%|██████████| 1366/1366 [00:12<00:00, 107.48it/s, loss=6.18e+4, metrics={'r2': 0.8276}]\n",
      "epoch 39: 100%|██████████| 1366/1366 [00:12<00:00, 108.48it/s, loss=6.13e+4, metrics={'r2': 0.83}]  \n",
      "epoch 40: 100%|██████████| 1366/1366 [00:12<00:00, 112.91it/s, loss=6.13e+4, metrics={'r2': 0.8301}]\n",
      "epoch 41: 100%|██████████| 1366/1366 [00:12<00:00, 112.98it/s, loss=6.14e+4, metrics={'r2': 0.8295}]\n",
      "epoch 42: 100%|██████████| 1366/1366 [00:12<00:00, 110.25it/s, loss=6.12e+4, metrics={'r2': 0.8308}]\n",
      "epoch 43: 100%|██████████| 1366/1366 [00:12<00:00, 112.28it/s, loss=6.09e+4, metrics={'r2': 0.8323}]\n",
      "epoch 44: 100%|██████████| 1366/1366 [00:12<00:00, 112.25it/s, loss=6.07e+4, metrics={'r2': 0.8334}]\n",
      "epoch 45: 100%|██████████| 1366/1366 [00:12<00:00, 111.83it/s, loss=6.07e+4, metrics={'r2': 0.833}] \n",
      "epoch 46: 100%|██████████| 1366/1366 [00:12<00:00, 112.32it/s, loss=6.06e+4, metrics={'r2': 0.8337}]\n",
      "epoch 47: 100%|██████████| 1366/1366 [00:12<00:00, 109.98it/s, loss=6.07e+4, metrics={'r2': 0.8328}]\n",
      "epoch 48: 100%|██████████| 1366/1366 [00:12<00:00, 111.24it/s, loss=6.04e+4, metrics={'r2': 0.8345}]\n",
      "epoch 49: 100%|██████████| 1366/1366 [00:12<00:00, 111.24it/s, loss=6.01e+4, metrics={'r2': 0.8363}]\n",
      "epoch 50: 100%|██████████| 1366/1366 [00:12<00:00, 110.99it/s, loss=6.01e+4, metrics={'r2': 0.8365}]\n",
      "epoch 51: 100%|██████████| 1366/1366 [00:12<00:00, 110.19it/s, loss=6.01e+4, metrics={'r2': 0.8362}]\n",
      "epoch 52: 100%|██████████| 1366/1366 [00:12<00:00, 108.15it/s, loss=5.98e+4, metrics={'r2': 0.838}] \n",
      "epoch 53: 100%|██████████| 1366/1366 [00:12<00:00, 110.65it/s, loss=5.98e+4, metrics={'r2': 0.8373}]\n",
      "epoch 54: 100%|██████████| 1366/1366 [00:12<00:00, 110.52it/s, loss=5.97e+4, metrics={'r2': 0.8383}]\n",
      "epoch 55: 100%|██████████| 1366/1366 [00:12<00:00, 111.29it/s, loss=5.94e+4, metrics={'r2': 0.8398}]\n",
      "epoch 56: 100%|██████████| 1366/1366 [00:12<00:00, 110.12it/s, loss=5.92e+4, metrics={'r2': 0.8404}]\n",
      "epoch 57: 100%|██████████| 1366/1366 [00:12<00:00, 108.07it/s, loss=5.93e+4, metrics={'r2': 0.84}]  \n",
      "epoch 58: 100%|██████████| 1366/1366 [00:12<00:00, 110.22it/s, loss=5.91e+4, metrics={'r2': 0.841}] \n",
      "epoch 59: 100%|██████████| 1366/1366 [00:12<00:00, 110.75it/s, loss=5.91e+4, metrics={'r2': 0.8413}]\n",
      "epoch 60: 100%|██████████| 1366/1366 [00:12<00:00, 110.06it/s, loss=5.87e+4, metrics={'r2': 0.8431}]\n",
      "epoch 61: 100%|██████████| 1366/1366 [00:12<00:00, 107.77it/s, loss=5.88e+4, metrics={'r2': 0.8424}]\n",
      "epoch 62: 100%|██████████| 1366/1366 [00:12<00:00, 109.55it/s, loss=5.87e+4, metrics={'r2': 0.8433}]\n",
      "epoch 63: 100%|██████████| 1366/1366 [00:12<00:00, 109.64it/s, loss=5.85e+4, metrics={'r2': 0.8444}]\n",
      "epoch 64: 100%|██████████| 1366/1366 [00:12<00:00, 109.18it/s, loss=5.81e+4, metrics={'r2': 0.8462}]\n",
      "epoch 65: 100%|██████████| 1366/1366 [00:13<00:00, 100.51it/s, loss=5.82e+4, metrics={'r2': 0.8457}]\n",
      "epoch 66: 100%|██████████| 1366/1366 [00:14<00:00, 93.16it/s, loss=5.79e+4, metrics={'r2': 0.8475}] \n",
      "epoch 67: 100%|██████████| 1366/1366 [00:16<00:00, 84.73it/s, loss=5.74e+4, metrics={'r2': 0.8502}] \n",
      "epoch 68: 100%|██████████| 1366/1366 [00:12<00:00, 105.81it/s, loss=5.7e+4, metrics={'r2': 0.8521}] \n",
      "epoch 69: 100%|██████████| 1366/1366 [00:12<00:00, 110.48it/s, loss=5.67e+4, metrics={'r2': 0.8542}]\n",
      "epoch 70: 100%|██████████| 1366/1366 [00:13<00:00, 98.50it/s, loss=5.64e+4, metrics={'r2': 0.8556}] \n",
      "epoch 71: 100%|██████████| 1366/1366 [00:14<00:00, 96.33it/s, loss=5.57e+4, metrics={'r2': 0.8588}] \n",
      "epoch 72: 100%|██████████| 1366/1366 [00:12<00:00, 110.65it/s, loss=5.48e+4, metrics={'r2': 0.8633}]\n",
      "epoch 73: 100%|██████████| 1366/1366 [00:12<00:00, 110.70it/s, loss=5.42e+4, metrics={'r2': 0.8665}]\n",
      "epoch 74: 100%|██████████| 1366/1366 [00:12<00:00, 111.19it/s, loss=5.37e+4, metrics={'r2': 0.8693}]\n",
      "epoch 75: 100%|██████████| 1366/1366 [00:12<00:00, 109.66it/s, loss=5.32e+4, metrics={'r2': 0.8714}]\n",
      "epoch 76: 100%|██████████| 1366/1366 [00:12<00:00, 111.00it/s, loss=5.28e+4, metrics={'r2': 0.8734}]\n",
      "epoch 77: 100%|██████████| 1366/1366 [00:12<00:00, 110.08it/s, loss=5.25e+4, metrics={'r2': 0.8745}]\n",
      "epoch 78: 100%|██████████| 1366/1366 [00:12<00:00, 110.74it/s, loss=5.24e+4, metrics={'r2': 0.8752}]\n",
      "epoch 79: 100%|██████████| 1366/1366 [00:12<00:00, 110.30it/s, loss=5.21e+4, metrics={'r2': 0.8766}]\n",
      "epoch 80: 100%|██████████| 1366/1366 [00:12<00:00, 109.12it/s, loss=5.2e+4, metrics={'r2': 0.877}]  \n",
      "epoch 81: 100%|██████████| 1366/1366 [00:12<00:00, 110.71it/s, loss=5.18e+4, metrics={'r2': 0.8776}]\n",
      "epoch 82: 100%|██████████| 1366/1366 [00:12<00:00, 110.27it/s, loss=5.15e+4, metrics={'r2': 0.8791}]\n",
      "epoch 83: 100%|██████████| 1366/1366 [00:12<00:00, 110.56it/s, loss=5.17e+4, metrics={'r2': 0.8784}]\n",
      "epoch 84: 100%|██████████| 1366/1366 [00:12<00:00, 109.82it/s, loss=5.14e+4, metrics={'r2': 0.8799}]\n",
      "epoch 85: 100%|██████████| 1366/1366 [00:12<00:00, 106.72it/s, loss=5.13e+4, metrics={'r2': 0.88}]  \n",
      "epoch 86: 100%|██████████| 1366/1366 [00:14<00:00, 94.29it/s, loss=5.13e+4, metrics={'r2': 0.8803}] \n",
      "epoch 87: 100%|██████████| 1366/1366 [00:12<00:00, 109.48it/s, loss=5.12e+4, metrics={'r2': 0.8807}]\n",
      "epoch 88: 100%|██████████| 1366/1366 [00:12<00:00, 108.27it/s, loss=5.1e+4, metrics={'r2': 0.8819}] \n",
      "epoch 89: 100%|██████████| 1366/1366 [00:12<00:00, 108.18it/s, loss=5.1e+4, metrics={'r2': 0.8817}] \n",
      "epoch 90: 100%|██████████| 1366/1366 [00:12<00:00, 108.14it/s, loss=5.08e+4, metrics={'r2': 0.8823}]\n",
      "epoch 91: 100%|██████████| 1366/1366 [00:12<00:00, 109.87it/s, loss=5.07e+4, metrics={'r2': 0.8832}]\n",
      "epoch 92: 100%|██████████| 1366/1366 [00:12<00:00, 109.10it/s, loss=5.06e+4, metrics={'r2': 0.8833}]\n",
      "epoch 93: 100%|██████████| 1366/1366 [00:12<00:00, 106.56it/s, loss=5.07e+4, metrics={'r2': 0.8831}]\n",
      "epoch 94: 100%|██████████| 1366/1366 [00:12<00:00, 108.92it/s, loss=5.04e+4, metrics={'r2': 0.8846}]\n",
      "epoch 95: 100%|██████████| 1366/1366 [00:12<00:00, 108.80it/s, loss=5.03e+4, metrics={'r2': 0.8848}]\n",
      "epoch 96: 100%|██████████| 1366/1366 [00:12<00:00, 108.81it/s, loss=5.03e+4, metrics={'r2': 0.8847}]\n",
      "epoch 97: 100%|██████████| 1366/1366 [00:12<00:00, 105.15it/s, loss=5.03e+4, metrics={'r2': 0.885}] \n",
      "epoch 98: 100%|██████████| 1366/1366 [01:38<00:00, 13.91it/s, loss=5.02e+4, metrics={'r2': 0.8853}] \n",
      "epoch 99: 100%|██████████| 1366/1366 [00:13<00:00, 101.50it/s, loss=5.03e+4, metrics={'r2': 0.8848}]\n",
      "epoch 100: 100%|██████████| 1366/1366 [01:55<00:00, 11.78it/s, loss=5.02e+4, metrics={'r2': 0.8857}] \n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "categorical_columns = ['month', 'town', 'flat_model_type', 'storey_range']\n",
    "continuous_columns = ['dist_to_nearest_stn', 'dist_to_dhoby', 'degree_centrality', 'eigenvector_centrality', 'remaining_lease_years', 'floor_area_sqm']\n",
    "target = df_train['resale_price'].values\n",
    "\n",
    "tab_preprocessor = TabPreprocessor(\n",
    "    cat_embed_cols=categorical_columns, continuous_cols=continuous_columns\n",
    ")\n",
    "\n",
    "X_tab = tab_preprocessor.fit_transform(df_train)\n",
    "\n",
    "tab_mlp = TabMlp(\n",
    "    column_idx=tab_preprocessor.column_idx,\n",
    "    cat_embed_input=tab_preprocessor.cat_embed_input,\n",
    "    continuous_cols=continuous_columns,\n",
    "    mlp_hidden_dims=[200, 100]\n",
    ")\n",
    "\n",
    "model = WideDeep(deeptabular=tab_mlp)\n",
    "trainer = Trainer(model, cost_function=\"root_mean_squared_error\", metrics=[R2Score], num_workers=0)\n",
    "trainer.fit(\n",
    "    X_tab=X_tab,\n",
    "    target=target,\n",
    "    n_epochs=100,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V46s-MdM0y5c"
   },
   "source": [
    ">Report the test RMSE and the test R2 value that you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "KAhAgvMC07g6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 1128/1128 [00:03<00:00, 283.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 97492.10034406491\n",
      "R2: 0.6108051271090131\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X_tab_test = tab_preprocessor.transform(df_test)\n",
    "y_pred = trainer.predict(X_tab=X_tab_test, batch_size=64)\n",
    "\n",
    "y_test = df_test['resale_price'].values\n",
    "rmse = mean_squared_error(y_pred, y_test, squared=False)\n",
    "\n",
    "r2 = r2_score(y_pred, y_test)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
