{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d01f9669-6940-4311-bcf9-e930ba3457f2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "d01f9669-6940-4311-bcf9-e930ba3457f2",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ad988175a8c2593a35c27c5a89d6ea5",
     "grade": false,
     "grade_id": "a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Question A1 (15 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd870a1-6368-4b4f-9cbb-7eac0acda4ec",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2fd870a1-6368-4b4f-9cbb-7eac0acda4ec",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "74435a282dd0f8cb4705e88829506c4e",
     "grade": false,
     "grade_id": "a1_overall",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Design a feedforward deep neural network (DNN) which consists of **three** hidden layers of 128 neurons each with ReLU activation function, and an output layer with sigmoid activation function. Apply dropout of probability **0.2** to each of the hidden layers.\n",
    "\n",
    "* note: some cells are non-editable and cannot be filled, but leave them untouched. Fill up only cells which are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af871c6-6baa-4eff-b46a-eb7a81faac13",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3af871c6-6baa-4eff-b46a-eb7a81faac13",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "909acb3c7ff3883eb5381eb586615d3b",
     "grade": false,
     "grade_id": "libraries",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.io import wavfile as wav\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "from common_utils import set_seed\n",
    "\n",
    "# setting seed\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27828402-6f73-4afc-ad4b-d537b94406fc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "27828402-6f73-4afc-ad4b-d537b94406fc",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "238677075e1bc63851fa208a1937b0d0",
     "grade": false,
     "grade_id": "a1_1_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "1. Define the model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0a262dc-01fc-42d3-9554-a2a9b41db5f6",
   "metadata": {
    "deletable": false,
    "id": "f0a262dc-01fc-42d3-9554-a2a9b41db5f6",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ffe9e91761659732c1cda9441c248079",
     "grade": false,
     "grade_id": "mlp",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, no_features, no_hidden, no_labels):\n",
    "        super().__init__()\n",
    "        self.mlp_stack = nn.Sequential(\n",
    "            nn.Linear(no_features, no_hidden),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(no_hidden, no_hidden),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(no_hidden, no_hidden),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(no_hidden, no_labels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    def forward(self, x):\n",
    "        return self.mlp_stack(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c508f1a3-1781-403e-a6c8-2133c83cf2c0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c508f1a3-1781-403e-a6c8-2133c83cf2c0",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8fa5e10c9346a8cc9f313cfdeffb29ad",
     "grade": false,
     "grade_id": "a1_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Divide the dataset into a 70:30 ratio for training and testing. Use **appropriate** scaling of input features. We solely assume that there are only two datasets here: training & test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95fca82-4f7a-4017-8990-e36a78db1560",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e95fca82-4f7a-4017-8990-e36a78db1560",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b42952eb98d955700c9f15482aa73f1",
     "grade": false,
     "grade_id": "a1_2_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "1. Split the dataset and do preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5fa518e-36fc-4071-839f-65766fe06f67",
   "metadata": {
    "deletable": false,
    "id": "c5fa518e-36fc-4071-839f-65766fe06f67",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f3aaa6c1bc49a791e2d944fbffa3785",
     "grade": false,
     "grade_id": "preprocess",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "pos    6202\n",
      "neg    5855\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from common_utils import split_dataset, preprocess_dataset\n",
    "\n",
    "random_seed = 0\n",
    "\n",
    "def preprocess(df):\n",
    "    # YOUR CODE HERE\n",
    "    X_train, y_train, X_test, y_test = split_dataset(df, ['filename', 'label'], 0.3, random_seed)\n",
    "    X_train_scaled, X_test_scaled = preprocess_dataset(X_train, X_test)\n",
    "    \n",
    "    return X_train_scaled, y_train, X_test_scaled, y_test\n",
    "\n",
    "df = pd.read_csv('simplified.csv')\n",
    "df['label'] = df['filename'].str.split('_').str[-2]\n",
    "\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "X_train_scaled, y_train, X_test_scaled, y_test = preprocess(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a819454c-0064-4d88-a35d-b54a3027f52f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "a819454c-0064-4d88-a35d-b54a3027f52f",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "58c39624353d30c202964d96c43a724f",
     "grade": false,
     "grade_id": "a1_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Use the training dataset to train the model for 100 epochs. Use a mini-batch gradient descent with **‘Adam’** optimizer with learning rate of **0.001**, and **batch size = 256**. Implement early stopping with patience of **3**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc79c097-e526-474a-803f-354465ac02d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fc79c097-e526-474a-803f-354465ac02d7",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45710c8274bd2b7689ccd170af7be67c",
     "grade": false,
     "grade_id": "a1_3_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "1. Define a Pytorch Dataset and Dataloaders.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "578b3050-9720-432b-9ef0-c29feba29e68",
   "metadata": {
    "deletable": false,
    "id": "578b3050-9720-432b-9ef0-c29feba29e68",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "808e279fafed9a2b06f06e824dc5fffe",
     "grade": false,
     "grade_id": "pytorch_dataset",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        assert len(features) == len(labels)\n",
    "\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        # model output shape is (X, 1), so we need to reshape our labels from 1D to 2D\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32).reshape(-1, 1) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        features = self.features[idx]\n",
    "\n",
    "        return features, label\n",
    "\n",
    "def intialise_loaders(X_train_scaled, y_train, X_test_scaled, y_test):\n",
    "    train_data = CustomDataset(X_train_scaled, y_train)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "\n",
    "    test_data = CustomDataset(X_test_scaled, y_test)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=256, shuffle=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "train_dataloader, test_dataloader = intialise_loaders(X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648cceb4-adb2-4e4d-bcc5-6cb9f9e252c6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "648cceb4-adb2-4e4d-bcc5-6cb9f9e252c6",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18d61ee1922cd394e81ef773a74489eb",
     "grade": false,
     "grade_id": "a1_3_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "2. Next, define the model, optimizer and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00e5365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(77, 128, 1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e178c6-bc5a-4932-9a0a-9a8ab5189c78",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "20e178c6-bc5a-4932-9a0a-9a8ab5189c78",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7831e2b9dd6b053fce7ebc058a3c5574",
     "grade": false,
     "grade_id": "a1_3_5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "3. Train model for 100 epochs. Record down train and test accuracies. Implement early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57924156-c046-4610-b4dd-cebeb683a6d5",
   "metadata": {
    "deletable": false,
    "id": "57924156-c046-4610-b4dd-cebeb683a6d5",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94cfe370d3e2676874c81aa42b52b1e1",
     "grade": true,
     "grade_id": "training",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training loss: 0.6779976071733417, Training acc: 0.5791227691101305, Test loss: 0.6802093982696533, Test acc: 0.5706188718477885\n",
      "Epoch 10, Training loss: 0.4932178764632254, Training acc: 0.764761417201071, Test loss: 0.5949139674504598, Test acc: 0.6867800235748291\n",
      "Early stop at epoch 19\n"
     ]
    }
   ],
   "source": [
    "from common_utils import EarlyStopper\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "\n",
    "accuracies_train = []\n",
    "accuracies_test = []\n",
    "\n",
    "early_stopper = EarlyStopper(patience=3)\n",
    "\n",
    "for i in range(epochs):\n",
    "    # prepare model for training\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_dataloader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval() # prepare model for evaluation\n",
    "    with torch.no_grad():\n",
    "        overall_loss = []\n",
    "        overall_accuracy = []\n",
    "\n",
    "        for X_batch, y_batch in train_dataloader:\n",
    "            y_pred = model.forward(X_batch)\n",
    "\n",
    "            accuracy = (y_pred.round() == y_batch).float().mean()\n",
    "            accuracy = float(accuracy)\n",
    "            overall_accuracy.append(accuracy)\n",
    "\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            overall_loss.append(loss.detach().numpy())\n",
    "\n",
    "        losses_train.append(sum(overall_loss) / len(overall_loss))\n",
    "        accuracies_train.append(sum(overall_accuracy) / len(overall_accuracy))\n",
    "\n",
    "        overall_loss = []\n",
    "        overall_accuracy = []\n",
    "\n",
    "        for X_batch, y_batch in test_dataloader:\n",
    "            y_pred = model.forward(X_batch)\n",
    "\n",
    "            accuracy = (y_pred.round() == y_batch).float().mean()\n",
    "            accuracy = float(accuracy)\n",
    "            overall_accuracy.append(accuracy)\n",
    "\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            overall_loss.append(loss.detach().numpy())\n",
    "\n",
    "        losses_test.append(sum(overall_loss) / len(overall_loss))\n",
    "        accuracies_test.append(sum(overall_accuracy) / len(overall_accuracy))\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Epoch {i}, Training loss: {losses_train[-1]}, Training acc: {accuracies_train[-1]}, Test loss: {losses_test[-1]}, Test acc: {accuracies_test[-1]}\")\n",
    "\n",
    "    if early_stopper.early_stop(losses_test[-1]):\n",
    "        print(f\"Early stop at epoch {i}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deafd80-4525-4bff-8a5f-dfee0bfede04",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5deafd80-4525-4bff-8a5f-dfee0bfede04",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f0d29724a79a38566c9153287e31469",
     "grade": false,
     "grade_id": "a1_4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Plot train and test accuracies and losses on training and test data against training epochs and comment on the line plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b25bf720-a4e3-4e70-b90c-456180f18dcf",
   "metadata": {
    "deletable": false,
    "id": "b25bf720-a4e3-4e70-b90c-456180f18dcf",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9ce6934b35dea460dcc15bb9a6dedf4",
     "grade": true,
     "grade_id": "plot",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tohhongxiang/Desktop/programming/CZ4042-Assignment/PartA_1.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tohhongxiang/Desktop/programming/CZ4042-Assignment/PartA_1.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tohhongxiang/Desktop/programming/CZ4042-Assignment/PartA_1.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m losses\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tohhongxiang/Desktop/programming/CZ4042-Assignment/PartA_1.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(losses_train, label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining loss\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tohhongxiang/Desktop/programming/CZ4042-Assignment/PartA_1.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(losses_test, label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTesting loss\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "losses\n",
    "plt.plot(losses_train, label=\"Training loss\")\n",
    "plt.plot(losses_test, label=\"Testing loss\")\n",
    "plt.plot(accuracies_train, label=\"Training accuracy\")\n",
    "plt.plot(accuracies_test, label=\"Testing accuracy\")\n",
    "\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3307014b-6a6c-406e-a139-94acc919f5f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3307014b-6a6c-406e-a139-94acc919f5f4",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0973f053aaa2d1bde7a2261c1d8cd9c4",
     "grade": false,
     "grade_id": "a1_4_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "2. Comment on line plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71af7e2-6ed7-4457-940a-df915d349ee0",
   "metadata": {
    "deletable": false,
    "id": "b71af7e2-6ed7-4457-940a-df915d349ee0",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2a6f6433369930987c9e7342cb667b0",
     "grade": false,
     "grade_id": "comment_plots",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "- The model quickly overfits, as shown in the plots. The training loss goes down, but the validation loss goes back up instead (which is a sign of overfitting)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
